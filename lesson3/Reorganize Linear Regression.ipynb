{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(w,b,x):\n",
    "#     print(x*w)\n",
    "    pred_y = x*w +b\n",
    "    return pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_loss(w,b,x_list,gt_y_list):\n",
    "    avg_loss = 0.5*(x_list*w + b - gt_y_list) ** 2\n",
    "#     print(avg_loss)\n",
    "    avg_loss = np.sum(avg_loss) / len(gt_y_list)\n",
    "    return avg_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(pred_y,gt_y,x):\n",
    "    diff = pred_y - gt_y\n",
    "    dw = np.multiply(diff,x)\n",
    "    db = diff\n",
    "    return dw,db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_step_gradient(batch_x_list,batch_gt_y_list,w,b,lr):\n",
    "    pred_y = inference(w,b,batch_x_list)\n",
    "    dw,db = gradient(pred_y,batch_gt_y_list,batch_x_list)\n",
    "    avg_dw = np.sum(dw) / len(batch_x_list)\n",
    "    avg_db = np.sum(db) / len(batch_x_list)\n",
    "    w -= lr * avg_dw\n",
    "    b -= lr * avg_db\n",
    "    return w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x_list, gt_y_list, batch_size, lr, max_iter):\n",
    "    w = 0\n",
    "    b = 0\n",
    "    num_samples = len(x_list)\n",
    "    for i in range(max_iter):\n",
    "        batch_idxs = np.random.choice(len(x_list), batch_size)\n",
    "        batch_x = np.array([x_list[j] for j in batch_idxs])\n",
    "        batch_y = np.array([gt_y_list[j] for j in batch_idxs])\n",
    "        w, b = cal_step_gradient(batch_x, batch_y, w, b, lr)\n",
    "        print('w:{0}, b:{1}'.format(w, b))\n",
    "        print('loss is {0}'.format(eval_loss(w, b, np.array(x_list), np.array(gt_y_list))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_sample_data():\n",
    "    w = random.randint(0, 10) + random.random()\n",
    "    b = random.randint(0, 5) + random.random()\n",
    "    num_samples = 100\n",
    "    x_list = []\n",
    "    y_list = []\n",
    "    for i in range(num_samples):\n",
    "        x = random.randint(0, 100) * random.random()\n",
    "        y = w * x + b + random.random() * random.randint(-1, 1)\n",
    "        x_list.append(x)\n",
    "        y_list.append(y)\n",
    "    return x_list, y_list, w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    x_list, y_list, w, b = gen_sample_data()\n",
    "#     print(x_list)\n",
    "    lr = 0.0001\n",
    "    max_iter = 100\n",
    "    train(x_list, y_list, 50, lr, max_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w:0.36418895218659586, b:0.009813758264450603\n",
      "loss is 4997.531289254207\n",
      "w:0.5880389953815419, b:0.01676221368620081\n",
      "loss is 4399.952665137066\n",
      "w:0.9484212746303915, b:0.025487748219898177\n",
      "loss is 3517.972326686571\n",
      "w:1.1356640039613626, b:0.030761251707623935\n",
      "loss is 3098.587096244481\n",
      "w:1.3955103397161317, b:0.037195265048434366\n",
      "loss is 2560.7239125632186\n",
      "w:1.561280172405808, b:0.042251910882925486\n",
      "loss is 2244.3152858077415\n",
      "w:1.722400102239599, b:0.04731167223565082\n",
      "loss is 1956.7664782554755\n",
      "w:1.8737930819932258, b:0.05173593657964803\n",
      "loss is 1704.5512678831385\n",
      "w:2.036130525796741, b:0.05642488657751328\n",
      "loss is 1453.4355038917777\n",
      "w:2.1504779667865184, b:0.05990756431490711\n",
      "loss is 1288.5551282003798\n",
      "w:2.2715078245127955, b:0.06374741248811132\n",
      "loss is 1124.846665384894\n",
      "w:2.397362981973976, b:0.06764801452004869\n",
      "loss is 966.4092096294646\n",
      "w:2.53870453581625, b:0.07117369309750922\n",
      "loss is 802.8377896794789\n",
      "w:2.588291450788221, b:0.0733203713488032\n",
      "loss is 749.0173019875048\n",
      "w:2.724217635055988, b:0.07686025033550493\n",
      "loss is 611.1275413338329\n",
      "w:2.805667757343443, b:0.07934281862171973\n",
      "loss is 535.2104214831842\n",
      "w:2.888638008706684, b:0.08176709564890078\n",
      "loss is 463.0569985379544\n",
      "w:2.983704821552781, b:0.08424348655032607\n",
      "loss is 386.8144814847915\n",
      "w:3.070041167794362, b:0.08646556093150838\n",
      "loss is 323.5174885238523\n",
      "w:3.1287898873657807, b:0.08827649159715628\n",
      "loss is 283.675104916755\n",
      "w:3.207427838479058, b:0.09032789742351086\n",
      "loss is 234.4513597340521\n",
      "w:3.2667405376026126, b:0.09204847577460167\n",
      "loss is 200.42659700501727\n",
      "w:3.3192283286491997, b:0.09346954402517908\n",
      "loss is 172.54562275057506\n",
      "w:3.364398686259095, b:0.09476127420439334\n",
      "loss is 150.22472311965103\n",
      "w:3.4097250852035565, b:0.09606120920350705\n",
      "loss is 129.38346246534616\n",
      "w:3.4539199792381865, b:0.09732143649185827\n",
      "loss is 110.56413368536083\n",
      "w:3.4923378483411653, b:0.09840988307933642\n",
      "loss is 95.40944019931823\n",
      "w:3.5261328031656216, b:0.09945501262867602\n",
      "loss is 83.00365488771745\n",
      "w:3.5576466635495643, b:0.10043782128907859\n",
      "loss is 72.21638251253589\n",
      "w:3.5831570938001076, b:0.10128097848799904\n",
      "loss is 64.03585682346699\n",
      "w:3.601403381195951, b:0.10204441121141244\n",
      "loss is 58.486454245562896\n",
      "w:3.62806806084685, b:0.10290096274400544\n",
      "loss is 50.83362419436772\n",
      "w:3.651271708919863, b:0.1036144419629073\n",
      "loss is 44.613612316544824\n",
      "w:3.6729343095312483, b:0.10428415589720066\n",
      "loss is 39.175587639579675\n",
      "w:3.6970931626265275, b:0.10500200400622582\n",
      "loss is 33.531306567665766\n",
      "w:3.7234127567488455, b:0.10571623696752624\n",
      "loss is 27.886886003210055\n",
      "w:3.753697334271503, b:0.10646398568612021\n",
      "loss is 22.04310393024493\n",
      "w:3.7719851859824307, b:0.10701413819143625\n",
      "loss is 18.850795471338913\n",
      "w:3.790250905020342, b:0.1075531549154561\n",
      "loss is 15.915824493266443\n",
      "w:3.805118205376466, b:0.1080097882848795\n",
      "loss is 13.713805087720798\n",
      "w:3.814720604086509, b:0.10837241272323204\n",
      "loss is 12.380482929735079\n",
      "w:3.832439869318291, b:0.1088525154634653\n",
      "loss is 10.104662315748572\n",
      "w:3.843666058524054, b:0.10923644532283408\n",
      "loss is 8.785828489307173\n",
      "w:3.8550374454674237, b:0.10958567092479295\n",
      "loss is 7.547626326323974\n",
      "w:3.863798818115862, b:0.10988927960844667\n",
      "loss is 6.660466412302037\n",
      "w:3.8752916019508614, b:0.11023124176517232\n",
      "loss is 5.585268858649323\n",
      "w:3.8843051123816923, b:0.11053040670346592\n",
      "loss is 4.8120875185693\n",
      "w:3.8907353372809905, b:0.1107696440672864\n",
      "loss is 4.298133532193501\n",
      "w:3.8990313536002628, b:0.11103692544091878\n",
      "loss is 3.6815401222344537\n",
      "w:3.905768846008286, b:0.11126982225385897\n",
      "loss is 3.2191960028705835\n",
      "w:3.913544473731689, b:0.11151960080287093\n",
      "loss is 2.7284962764590226\n",
      "w:3.9202310919260746, b:0.11174245778172734\n",
      "loss is 2.3432142504687214\n",
      "w:3.925073086878814, b:0.11193453681460057\n",
      "loss is 2.085352972377058\n",
      "w:3.929430359598306, b:0.11210986965796997\n",
      "loss is 1.8685235166831398\n",
      "w:3.935484145462032, b:0.11229969672870294\n",
      "loss is 1.591296169052204\n",
      "w:3.940548459128754, b:0.11247015036955038\n",
      "loss is 1.380735571483676\n",
      "w:3.9438634899068195, b:0.11261996747713425\n",
      "loss is 1.2533941244584952\n",
      "w:3.947124788860378, b:0.11276994209503592\n",
      "loss is 1.1362604235009746\n",
      "w:3.950385010753953, b:0.11291024711536007\n",
      "loss is 1.02725555808567\n",
      "w:3.9533293706665216, b:0.11304636621426127\n",
      "loss is 0.9357383519056962\n",
      "w:3.956631139274331, b:0.11318070378596441\n",
      "loss is 0.84096917292726\n",
      "w:3.9590017877977672, b:0.11330138870464278\n",
      "loss is 0.7780042688355919\n",
      "w:3.9615489748306056, b:0.11341448293137388\n",
      "loss is 0.7151290537396291\n",
      "w:3.9640134339566897, b:0.11353389499066342\n",
      "loss is 0.6589769523059213\n",
      "w:3.9661954259140364, b:0.11364329394177189\n",
      "loss is 0.6131091524480722\n",
      "w:3.9677959140484105, b:0.1137418236835903\n",
      "loss is 0.5817475239277872\n",
      "w:3.969460964544617, b:0.1138335535920093\n",
      "loss is 0.5511983944741753\n",
      "w:3.971182790459687, b:0.113917645472687\n",
      "loss is 0.5218339355228113\n",
      "w:3.9730950941111134, b:0.11401784803574867\n",
      "loss is 0.49185559225331843\n",
      "w:3.9746705694923485, b:0.11412224323599988\n",
      "loss is 0.4692274309521185\n",
      "w:3.9760008123752035, b:0.1142189829548717\n",
      "loss is 0.4515845557343829\n",
      "w:3.9768910766660026, b:0.11430625642357087\n",
      "loss is 0.44051128859619854\n",
      "w:3.977839079248864, b:0.11437612792219023\n",
      "loss is 0.4294016444572711\n",
      "w:3.9791195662734933, b:0.11444897251472076\n",
      "loss is 0.41549740478205016\n",
      "w:3.9801578828935678, b:0.11451946578467001\n",
      "loss is 0.40512970541238685\n",
      "w:3.980847734031269, b:0.11458914353968988\n",
      "loss is 0.39867868628480785\n",
      "w:3.981789963007922, b:0.1146682823958353\n",
      "loss is 0.3904646502312877\n",
      "w:3.982676718454286, b:0.11474230875102132\n",
      "loss is 0.38335194774177245\n",
      "w:3.9831372068725557, b:0.11480764645520021\n",
      "loss is 0.3798767000264003\n",
      "w:3.9837526319275627, b:0.11486741885112972\n",
      "loss is 0.3755029408285235\n",
      "w:3.984289381894551, b:0.11494386713940188\n",
      "loss is 0.37190839476538506\n",
      "w:3.98495483960132, b:0.1150025624783051\n",
      "loss is 0.36777998296947684\n",
      "w:3.9856060722704525, b:0.11506201734480251\n",
      "loss is 0.364065099395612\n",
      "w:3.985830006233121, b:0.11510533054696664\n",
      "loss is 0.3628484719052518\n",
      "w:3.9865207155980706, b:0.11518186806150056\n",
      "loss is 0.3593715720297327\n",
      "w:3.9871912796216655, b:0.11525028380133785\n",
      "loss is 0.3563474980013898\n",
      "w:3.987686202930944, b:0.11529735366132052\n",
      "loss is 0.35433727319596864\n",
      "w:3.988273967650227, b:0.11536150554082612\n",
      "loss is 0.35218778561022573\n",
      "w:3.9884567516526293, b:0.11542015313975262\n",
      "loss is 0.35155171176522615\n",
      "w:3.9884700468978456, b:0.11547155245251688\n",
      "loss is 0.35148050420145444\n",
      "w:3.988747416754023, b:0.11551839183125012\n",
      "loss is 0.35059058664216897\n",
      "w:3.9896360236200175, b:0.11558234405261561\n",
      "loss is 0.34818113644914866\n",
      "w:3.9893731383941105, b:0.11563258007269613\n",
      "loss is 0.34879461397675854\n",
      "w:3.9897331316541766, b:0.11569338770899698\n",
      "loss is 0.3478997304216972\n",
      "w:3.989362680718293, b:0.11573739093508356\n",
      "loss is 0.34876543525027254\n",
      "w:3.9898560128294087, b:0.1158055502034726\n",
      "loss is 0.347569320781992\n",
      "w:3.990197198254244, b:0.11586172344390815\n",
      "loss is 0.34684616832096343\n",
      "w:3.991022912112137, b:0.11592222077931047\n",
      "loss is 0.3455019533757037\n",
      "w:3.991256005760099, b:0.11597539131982333\n",
      "loss is 0.34519864122458116\n",
      "w:3.9916035376551133, b:0.11603270611810164\n",
      "loss is 0.3448344848714956\n"
     ]
    }
   ],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
