{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(w,b,x):\n",
    "    pred_y = w * x + b\n",
    "    return pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_loss(w,b,x_list,y_list):\n",
    "    avg_loss = 0.0\n",
    "    for i in range(len(x_list)):\n",
    "        avg_loss += 0.5 * (w * x_list[i] + b - y_list[i]) ** 2\n",
    "    avg_loss /= len(y_list)\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(pred_y,gt_y,x):\n",
    "    diff = pred_y - gt_y\n",
    "    dw = diff * x\n",
    "    db = diff\n",
    "    return dw,db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_step_gradient(batch_x_list,batch_gt_y,w,b,lr):\n",
    "    #batch:æ‰¹\n",
    "    avg_dw,avg_db = 0,0\n",
    "    batch_size = len(batch_x_list)\n",
    "    for i in range(batch_size):\n",
    "        pred_y = inference(w,b,batch_x_list[i])\n",
    "        dw,db = gradient(pred_y,batch_gt_y[i],batch_x_list[i])\n",
    "        avg_dw += dw\n",
    "        avg_db += db\n",
    "    avg_dw /= batch_size\n",
    "    avg_db /= batch_size\n",
    "    w -= lr * avg_dw\n",
    "    b -= lr * avg_db\n",
    "    return w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x_list,gt_y_list,batch_size,lr,max_iter):\n",
    "    w = 0\n",
    "    b = 0\n",
    "    num_samples = len(x_list)\n",
    "    for i in range(max_iter):\n",
    "        batch_idxs = np.random.choice(len(x_list),batch_size)\n",
    "        batch_x = [x_list[j] for j in batch_idxs]\n",
    "        batch_y = [gt_y_list[j] for j in batch_idxs]\n",
    "        w,b = cal_step_gradient(batch_x,batch_y,w,b,lr)\n",
    "        print('w:{0},b:{1}'.format(w,b))\n",
    "        print('loss is {0}'.format(eval_loss(w,b,x_list,gt_y_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_sample_data():\n",
    "    w = random.randint(0,10) + random.random()\n",
    "    b = random.randint(0,5) + random.random()\n",
    "    num_samples = 100\n",
    "    x_list = []\n",
    "    y_list = []\n",
    "    for i in range(num_samples):\n",
    "        x = random.randint(0,100)*random.random()\n",
    "        y = w * x + b + random.random() * random.randint(-1,1)\n",
    "        x_list.append(x)\n",
    "        y_list.append(y)\n",
    "    return x_list,y_list,w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    x_list,y_list,w,b = gen_sample_data()\n",
    "    lr = 0.001\n",
    "    max_iter = 100\n",
    "    print('true w and b is ',w,b)\n",
    "    train(x_list,y_list,50,lr,max_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true w and b is  9.729019257609163 0.9100236449243333\n",
      "w:8.90598243940154,b:0.22852052210153012\n",
      "loss is 335.3237287103813\n",
      "w:9.693365002579776,b:0.24716905967495187\n",
      "loss is 1.5206561872012332\n",
      "w:9.761355457890026,b:0.2487410300942076\n",
      "loss is 0.3205253292232675\n",
      "w:9.745119223014209,b:0.24868847799667673\n",
      "loss is 0.2078935318973745\n",
      "w:9.747209754292955,b:0.2490995853067752\n",
      "loss is 0.2082446139723347\n",
      "w:9.744890839116534,b:0.24936409923524638\n",
      "loss is 0.20786168258826407\n",
      "w:9.746639336388576,b:0.24968816814113853\n",
      "loss is 0.20752050063873498\n",
      "w:9.745094066989104,b:0.2499528625368615\n",
      "loss is 0.20748560263282342\n",
      "w:9.74880269315598,b:0.2502640938591721\n",
      "loss is 0.21110345545222425\n",
      "w:9.74379350787406,b:0.25042473033061136\n",
      "loss is 0.2091073433218833\n",
      "w:9.74264778068936,b:0.2507068961868959\n",
      "loss is 0.21189581293612916\n",
      "w:9.74705361143072,b:0.2510336324222579\n",
      "loss is 0.2074954292249252\n",
      "w:9.743015348928845,b:0.2513717526827079\n",
      "loss is 0.21057843170690124\n",
      "w:9.747115650977722,b:0.2517766154017889\n",
      "loss is 0.20735008970919128\n",
      "w:9.75008411806798,b:0.2521608486980315\n",
      "loss is 0.214993783838119\n",
      "w:9.74532673179622,b:0.25241861034352264\n",
      "loss is 0.20651733541862632\n",
      "w:9.74812234782844,b:0.2528208189692898\n",
      "loss is 0.20875419030059925\n",
      "w:9.750168867522046,b:0.25321225997217567\n",
      "loss is 0.21510317441124183\n",
      "w:9.748551160345878,b:0.2536234978422769\n",
      "loss is 0.2095663298348074\n",
      "w:9.746980228646734,b:0.25383535298924487\n",
      "loss is 0.20659320061028882\n",
      "w:9.739245229815786,b:0.25392030137984045\n",
      "loss is 0.2263362516616894\n",
      "w:9.741966170773745,b:0.25428040184913103\n",
      "loss is 0.2127646499153705\n",
      "w:9.746329635707278,b:0.2547822520646384\n",
      "loss is 0.20578050985086357\n",
      "w:9.746921400397268,b:0.25518075171123494\n",
      "loss is 0.2061354565837113\n",
      "w:9.74890887418624,b:0.25549421608710554\n",
      "loss is 0.210090546163056\n",
      "w:9.746805360741481,b:0.2557446008686434\n",
      "loss is 0.20584980705112968\n",
      "w:9.742700251171826,b:0.2560136772871383\n",
      "loss is 0.20968434468537167\n",
      "w:9.74129884548377,b:0.2563512476831825\n",
      "loss is 0.2145418049677534\n",
      "w:9.745262441432638,b:0.25671563593557517\n",
      "loss is 0.20513617756004135\n",
      "w:9.740598916473719,b:0.2569833630836833\n",
      "loss is 0.21745892014579193\n",
      "w:9.745045538998356,b:0.257463567106839\n",
      "loss is 0.20500917081204478\n",
      "w:9.747188302632338,b:0.2578791072090346\n",
      "loss is 0.20569033721795507\n",
      "w:9.745222646126006,b:0.2581671805783168\n",
      "loss is 0.2046801013439646\n",
      "w:9.744208441145302,b:0.2585616141040133\n",
      "loss is 0.2054974369410385\n",
      "w:9.74350933028655,b:0.25885255403252155\n",
      "loss is 0.20661209305782346\n",
      "w:9.748002337563245,b:0.2592277604929724\n",
      "loss is 0.20678591569816374\n",
      "w:9.745914163188427,b:0.25957455448348976\n",
      "loss is 0.20415165817602074\n",
      "w:9.735993307298784,b:0.2596585335726948\n",
      "loss is 0.24855488629474715\n",
      "w:9.744885973515908,b:0.26023690813922545\n",
      "loss is 0.20420000857342024\n",
      "w:9.745474797181222,b:0.26078183731219257\n",
      "loss is 0.20375856504969633\n",
      "w:9.748398630218084,b:0.26110460822442494\n",
      "loss is 0.20725453123424564\n",
      "w:9.742161420788973,b:0.2612854477973342\n",
      "loss is 0.20931251240254178\n",
      "w:9.74626581026265,b:0.2617240156700637\n",
      "loss is 0.20364299045870637\n",
      "w:9.747700382196172,b:0.2620766950141052\n",
      "loss is 0.20539668566835542\n",
      "w:9.744806673484918,b:0.2623090986016747\n",
      "loss is 0.2035722639414736\n",
      "w:9.741257585255385,b:0.262444641284425\n",
      "loss is 0.21220467358548523\n",
      "w:9.743220898172252,b:0.26275757856790183\n",
      "loss is 0.2058127733815386\n",
      "w:9.744123157276613,b:0.2631192036739629\n",
      "loss is 0.20403580971251717\n",
      "w:9.745761457500492,b:0.26344062797833134\n",
      "loss is 0.202924657791885\n",
      "w:9.748173694445347,b:0.2637836139658256\n",
      "loss is 0.2060001418774668\n",
      "w:9.739040303566536,b:0.26401667779155097\n",
      "loss is 0.22297350513572936\n",
      "w:9.74670876405288,b:0.2644616090789951\n",
      "loss is 0.20321442919523947\n",
      "w:9.749828019369382,b:0.26488659509800294\n",
      "loss is 0.21113224015230142\n",
      "w:9.746412460851356,b:0.26508642343145167\n",
      "loss is 0.20275012874613754\n",
      "w:9.740611918268641,b:0.265268387358921\n",
      "loss is 0.21387671252985904\n",
      "w:9.740232244327041,b:0.26558851459745186\n",
      "loss is 0.2155845494057775\n",
      "w:9.744746787286418,b:0.2659621087269014\n",
      "loss is 0.20241083180009256\n",
      "w:9.747500041352083,b:0.26635391284043997\n",
      "loss is 0.20386316248786177\n",
      "w:9.747786645791896,b:0.2666627573815977\n",
      "loss is 0.20436027265557058\n",
      "w:9.753711663279692,b:0.26715744947160985\n",
      "loss is 0.2337460698379163\n",
      "w:9.739761175338112,b:0.2670992418524249\n",
      "loss is 0.21739863832078016\n",
      "w:9.74130358790273,b:0.2674872987191069\n",
      "loss is 0.20996990924406705\n",
      "w:9.747619161854033,b:0.2679750681703697\n",
      "loss is 0.20366458761249753\n",
      "w:9.752422949942385,b:0.2683136485979677\n",
      "loss is 0.22430798867184992\n",
      "w:9.746144962470511,b:0.2685140208093366\n",
      "loss is 0.20154460195905674\n",
      "w:9.747200650916502,b:0.268825310730401\n",
      "loss is 0.2026664939635296\n",
      "w:9.746057721342124,b:0.26903067483294624\n",
      "loss is 0.2013384804817424\n",
      "w:9.746816399623997,b:0.26930145760008484\n",
      "loss is 0.20196864445124155\n",
      "w:9.743239009525482,b:0.2695808991672467\n",
      "loss is 0.2033019517147064\n",
      "w:9.746167635583305,b:0.2698983629665284\n",
      "loss is 0.20115149671116883\n",
      "w:9.742026242899632,b:0.27014093220871943\n",
      "loss is 0.20632123978786443\n",
      "w:9.742167199989153,b:0.27051064707611855\n",
      "loss is 0.20573430778501933\n",
      "w:9.744290002113292,b:0.27079444334901126\n",
      "loss is 0.20122100802029197\n",
      "w:9.746609601426169,b:0.2710370235855161\n",
      "loss is 0.2012279109423794\n",
      "w:9.739524542742007,b:0.2711751111214718\n",
      "loss is 0.21691823837821023\n",
      "w:9.746862962916465,b:0.27161060768584666\n",
      "loss is 0.2013877774744196\n",
      "w:9.747607622312938,b:0.2718773411498665\n",
      "loss is 0.20261758978378744\n",
      "w:9.74731842729221,b:0.2721491025850113\n",
      "loss is 0.20197636010942357\n",
      "w:9.75099034423628,b:0.27255736147327514\n",
      "loss is 0.21503769233849857\n",
      "w:9.747514305753134,b:0.27285133704378384\n",
      "loss is 0.20217026450301323\n",
      "w:9.750099724028447,b:0.2731795830453603\n",
      "loss is 0.21054068767655168\n",
      "w:9.746115381297418,b:0.27338951318412225\n",
      "loss is 0.20009232751891712\n",
      "w:9.744567990969758,b:0.2735793611482215\n",
      "loss is 0.20005066843728392\n",
      "w:9.745143588454011,b:0.27389642288430194\n",
      "loss is 0.1996810624435227\n",
      "w:9.741949121299216,b:0.27421271687910687\n",
      "loss is 0.20500833518010034\n",
      "w:9.743382238638208,b:0.2745507597058304\n",
      "loss is 0.20125788916422727\n",
      "w:9.742477524857748,b:0.27488520088127905\n",
      "loss is 0.20319334552400037\n",
      "w:9.747794203784801,b:0.2753213145127641\n",
      "loss is 0.2021500097719769\n",
      "w:9.745271744552914,b:0.2756251161302659\n",
      "loss is 0.19913074241931583\n",
      "w:9.746550294410381,b:0.27593086510894643\n",
      "loss is 0.1997820979708221\n",
      "w:9.748127005057935,b:0.2763724838553685\n",
      "loss is 0.20272933063624962\n",
      "w:9.743548114556745,b:0.27659945120427026\n",
      "loss is 0.2002561312672902\n",
      "w:9.742278882652096,b:0.27687994095227847\n",
      "loss is 0.20300323343924248\n",
      "w:9.744709066350662,b:0.2772841143413034\n",
      "loss is 0.19876805898799607\n",
      "w:9.743207070314874,b:0.27753213690829964\n",
      "loss is 0.20054403749969046\n",
      "w:9.745663988673815,b:0.2778253433569892\n",
      "loss is 0.1985357521149744\n",
      "w:9.744278400563532,b:0.27811312495966495\n",
      "loss is 0.19881174132699464\n",
      "w:9.74453972191338,b:0.27842294061461464\n",
      "loss is 0.19850462018705595\n",
      "w:9.742471300323096,b:0.2787252348774178\n",
      "loss is 0.20179616242401135\n",
      "w:9.748426646751918,b:0.27913249919949834\n",
      "loss is 0.2029268605206344\n"
     ]
    }
   ],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
